"""
í†µí•© í…ŒìŠ¤íŠ¸ - ì „ì²´ ì‹œìŠ¤í…œ ê²€ì¦
"""

import pytest
import asyncio
from typing import List, Dict, Any
from datetime import datetime

from ..services.category_optimizer import CategoryOptimizer
from ..services.paper_quality_evaluator import PaperQualityEvaluator, QualityGrade
from ..services.cache_manager import CacheManager
from ..services.content_generators.shorts_generator import ShortsScriptGenerator
from ..services.content_generators.article_generator import ArticleGenerator
from ..services.content_generators.report_generator import ReportGenerator
from ..services.thinking.thinking_integration import ThinkingEnabledContentGenerator
from ..services.thinking.prompt_engineering import ContentType

class IntegrationTestSuite:
    """í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        self.category_optimizer = CategoryOptimizer()
        self.paper_evaluator = PaperQualityEvaluator()
        self.cache_manager = CacheManager()
        self.thinking_generator = ThinkingEnabledContentGenerator()
        self.test_results = []
        
    async def run_all_tests(self):
        """ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ§ª í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        print("=" * 80)
        
        # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë“¤
        test_scenarios = [
            self.test_category_to_content_flow,
            self.test_paper_quality_integration,
            self.test_cache_performance,
            self.test_thinking_mode_integration,
            self.test_multi_format_generation,
            self.test_error_handling,
            self.test_performance_benchmarks
        ]
        
        for test_func in test_scenarios:
            await test_func()
            
        # ê²°ê³¼ ìš”ì•½
        self._print_test_summary()
        
    async def test_category_to_content_flow(self):
        """ì¹´í…Œê³ ë¦¬ ìƒì„±ë¶€í„° ì½˜í…ì¸  ìƒì„±ê¹Œì§€ ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ 1: ì¹´í…Œê³ ë¦¬ â†’ ì½˜í…ì¸  ì „ì²´ í”Œë¡œìš°")
        print("-" * 60)
        
        start_time = datetime.now()
        
        try:
            # 1. ì¹´í…Œê³ ë¦¬ ìƒì„±
            categories = await self.category_optimizer.generate_categories("ë³µë¶€ ìš´ë™")
            assert len(categories) > 0, "ì¹´í…Œê³ ë¦¬ ìƒì„± ì‹¤íŒ¨"
            
            selected_category = categories[0]
            print(f"âœ… ì¹´í…Œê³ ë¦¬ ìƒì„±: {selected_category['name']}")
            
            # 2. ë…¼ë¬¸ ê²€ìƒ‰ ë° í‰ê°€
            papers = self._get_mock_papers()
            evaluated_papers = []
            
            for paper in papers:
                quality_info = self.paper_evaluator.evaluate_paper(paper)
                if quality_info.grade in [QualityGrade.A_PLUS, QualityGrade.A, QualityGrade.B_PLUS]:
                    evaluated_papers.append(paper)
                    
            print(f"âœ… ê³ í’ˆì§ˆ ë…¼ë¬¸ ì„ ë³„: {len(evaluated_papers)}ê°œ")
            
            # 3. ì½˜í…ì¸  ìƒì„±
            result = await self.thinking_generator.generate_with_thinking(
                content_type=ContentType.SHORTS,
                topic=selected_category['name'],
                papers=evaluated_papers,
                target_audience='general'
            )
            
            assert result['content'] is not None, "ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨"
            print(f"âœ… ì½˜í…ì¸  ìƒì„± ì™„ë£Œ (í’ˆì§ˆ: {result['content'].quality_score:.1f}/100)")
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            self.test_results.append({
                'test': 'category_to_content_flow',
                'success': True,
                'elapsed_time': elapsed,
                'details': {
                    'categories_generated': len(categories),
                    'papers_evaluated': len(papers),
                    'quality_papers': len(evaluated_papers),
                    'content_quality': result['content'].quality_score
                }
            })
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results.append({
                'test': 'category_to_content_flow',
                'success': False,
                'error': str(e)
            })
    
    async def test_paper_quality_integration(self):
        """ë…¼ë¬¸ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ 2: ë…¼ë¬¸ í’ˆì§ˆ í‰ê°€ í†µí•©")
        print("-" * 60)
        
        try:
            papers = self._get_diverse_papers()
            quality_distribution = {
                'A+': 0, 'A': 0, 'B+': 0, 'B': 0, 'C': 0, 'D': 0
            }
            
            for paper in papers:
                quality_info = self.paper_evaluator.evaluate_paper(paper)
                quality_distribution[quality_info.grade.value] += 1
                
            print("âœ… í’ˆì§ˆ ë“±ê¸‰ ë¶„í¬:")
            for grade, count in quality_distribution.items():
                if count > 0:
                    print(f"   - {grade} ë“±ê¸‰: {count}ê°œ")
                    
            # ê²€ì¦: ë‹¤ì–‘í•œ ë“±ê¸‰ì´ ë‚˜ì™”ëŠ”ì§€
            unique_grades = sum(1 for count in quality_distribution.values() if count > 0)
            assert unique_grades >= 3, "í’ˆì§ˆ í‰ê°€ê°€ ë„ˆë¬´ ë‹¨ìˆœí•¨"
            
            self.test_results.append({
                'test': 'paper_quality_integration',
                'success': True,
                'details': quality_distribution
            })
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results.append({
                'test': 'paper_quality_integration',
                'success': False,
                'error': str(e)
            })
    
    async def test_cache_performance(self):
        """ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ 3: ìºì‹œ ì„±ëŠ¥")
        print("-" * 60)
        
        try:
            # ìºì‹œ ì—†ì´ ìƒì„±
            topic = "HIIT ìš´ë™ë²•"
            
            start_no_cache = datetime.now()
            result1 = await self._generate_content_no_cache(topic)
            time_no_cache = (datetime.now() - start_no_cache).total_seconds()
            
            # ìºì‹œì— ì €ì¥
            cache_key = f"content_{topic}_shorts"
            self.cache_manager.set(cache_key, result1, ttl=3600)
            
            # ìºì‹œì—ì„œ ì½ê¸°
            start_with_cache = datetime.now()
            cached_result = self.cache_manager.get(cache_key)
            time_with_cache = (datetime.now() - start_with_cache).total_seconds()
            
            assert cached_result is not None, "ìºì‹œ ì½ê¸° ì‹¤íŒ¨"
            
            improvement = ((time_no_cache - time_with_cache) / time_no_cache) * 100
            print(f"âœ… ìºì‹œ ì—†ì´: {time_no_cache:.3f}ì´ˆ")
            print(f"âœ… ìºì‹œ ì‚¬ìš©: {time_with_cache:.3f}ì´ˆ")
            print(f"âœ… ì„±ëŠ¥ í–¥ìƒ: {improvement:.1f}%")
            
            self.test_results.append({
                'test': 'cache_performance',
                'success': True,
                'details': {
                    'time_no_cache': time_no_cache,
                    'time_with_cache': time_with_cache,
                    'improvement_percent': improvement
                }
            })
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results.append({
                'test': 'cache_performance',
                'success': False,
                'error': str(e)
            })
    
    async def test_thinking_mode_integration(self):
        """Native Thinking Mode í†µí•© í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ 4: Native Thinking Mode í†µí•©")
        print("-" * 60)
        
        try:
            papers = self._get_mock_papers()[:2]
            
            # Thinking Mode í™œì„±í™”
            result_with = await self.thinking_generator.generate_with_thinking(
                content_type=ContentType.ARTICLE,
                topic="ê·¼ìœ¡ íšŒë³µì„ ìœ„í•œ ì˜ì–‘ ì „ëµ",
                papers=papers,
                target_audience='general'
            )
            
            thinking_quality = result_with['thinking']['quality_score']
            content_quality = result_with['content'].quality_score
            
            print(f"âœ… ì‚¬ê³  í’ˆì§ˆ: {thinking_quality:.1f}/100")
            print(f"âœ… ì½˜í…ì¸  í’ˆì§ˆ: {content_quality:.1f}/100")
            print(f"âœ… ì‚¬ê³  íŒ¨í„´: {', '.join(result_with['thinking']['patterns'])}")
            
            assert thinking_quality >= 60, "ì‚¬ê³  í’ˆì§ˆì´ ë„ˆë¬´ ë‚®ìŒ"
            assert content_quality >= 70, "ì½˜í…ì¸  í’ˆì§ˆì´ ë„ˆë¬´ ë‚®ìŒ"
            
            self.test_results.append({
                'test': 'thinking_mode_integration',
                'success': True,
                'details': {
                    'thinking_quality': thinking_quality,
                    'content_quality': content_quality,
                    'thinking_patterns': result_with['thinking']['patterns']
                }
            })
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results.append({
                'test': 'thinking_mode_integration',
                'success': False,
                'error': str(e)
            })
    
    async def test_multi_format_generation(self):
        """ë©€í‹° í¬ë§· ìƒì„± í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ 5: ë©€í‹° í¬ë§· ìƒì„±")
        print("-" * 60)
        
        try:
            topic = "ì½”ì–´ ê°•í™” ìš´ë™ì˜ ì¤‘ìš”ì„±"
            papers = self._get_mock_papers()[:2]
            
            formats = [ContentType.SHORTS, ContentType.ARTICLE, ContentType.REPORT]
            results = {}
            
            for format_type in formats:
                result = await self.thinking_generator.generate_with_thinking(
                    content_type=format_type,
                    topic=topic,
                    papers=papers,
                    target_audience='general'
                )
                
                results[format_type.value] = {
                    'quality': result['content'].quality_score,
                    'length': len(result['content'].total_content)
                }
                
            print("âœ… ìƒì„± ê²°ê³¼:")
            for format_name, data in results.items():
                print(f"   - {format_name}: í’ˆì§ˆ {data['quality']:.1f}/100, ê¸¸ì´ {data['length']}ì")
                
            # ê° í¬ë§·ì´ ì ì ˆí•œ ê¸¸ì´ì¸ì§€ ê²€ì¦
            assert 200 <= results['shorts']['length'] <= 500, "ìˆì¸  ê¸¸ì´ ë¶€ì ì ˆ"
            assert 1500 <= results['article']['length'] <= 3500, "ì•„í‹°í´ ê¸¸ì´ ë¶€ì ì ˆ"
            assert 3000 <= results['report']['length'] <= 6000, "ë¦¬í¬íŠ¸ ê¸¸ì´ ë¶€ì ì ˆ"
            
            self.test_results.append({
                'test': 'multi_format_generation',
                'success': True,
                'details': results
            })
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results.append({
                'test': 'multi_format_generation',
                'success': False,
                'error': str(e)
            })
    
    async def test_error_handling(self):
        """ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ 6: ì—ëŸ¬ ì²˜ë¦¬")
        print("-" * 60)
        
        error_cases = []
        
        # ì¼€ì´ìŠ¤ 1: ë¹ˆ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        try:
            result = await self.thinking_generator.generate_with_thinking(
                content_type=ContentType.SHORTS,
                topic="í…ŒìŠ¤íŠ¸ ì£¼ì œ",
                papers=[],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
                target_audience='general'
            )
            error_cases.append(('empty_papers', 'handled'))
        except Exception:
            error_cases.append(('empty_papers', 'error'))
            
        # ì¼€ì´ìŠ¤ 2: ì˜ëª»ëœ ì½˜í…ì¸  íƒ€ì…
        try:
            result = await self.thinking_generator.generate_with_thinking(
                content_type="invalid_type",  # ì˜ëª»ëœ íƒ€ì…
                topic="í…ŒìŠ¤íŠ¸ ì£¼ì œ",
                papers=self._get_mock_papers(),
                target_audience='general'
            )
            error_cases.append(('invalid_type', 'handled'))
        except Exception:
            error_cases.append(('invalid_type', 'error'))
            
        print("âœ… ì—ëŸ¬ ì²˜ë¦¬ ê²°ê³¼:")
        for case, result in error_cases:
            print(f"   - {case}: {result}")
            
        self.test_results.append({
            'test': 'error_handling',
            'success': True,
            'details': dict(error_cases)
        })
    
    async def test_performance_benchmarks(self):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ 7: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("-" * 60)
        
        try:
            benchmarks = {}
            papers = self._get_mock_papers()[:3]
            
            # ê° ì½˜í…ì¸  íƒ€ì…ë³„ ìƒì„± ì‹œê°„ ì¸¡ì •
            for content_type in [ContentType.SHORTS, ContentType.ARTICLE, ContentType.REPORT]:
                start_time = datetime.now()
                
                result = await self.thinking_generator.generate_with_thinking(
                    content_type=content_type,
                    topic="ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸",
                    papers=papers,
                    target_audience='general'
                )
                
                elapsed = (datetime.now() - start_time).total_seconds()
                benchmarks[content_type.value] = elapsed
                
            print("âœ… ìƒì„± ì‹œê°„:")
            for content_type, time in benchmarks.items():
                print(f"   - {content_type}: {time:.2f}ì´ˆ")
                
            # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
            assert benchmarks['shorts'] < 5, "ìˆì¸  ìƒì„±ì´ ë„ˆë¬´ ëŠë¦¼"
            assert benchmarks['article'] < 10, "ì•„í‹°í´ ìƒì„±ì´ ë„ˆë¬´ ëŠë¦¼"
            assert benchmarks['report'] < 15, "ë¦¬í¬íŠ¸ ìƒì„±ì´ ë„ˆë¬´ ëŠë¦¼"
            
            self.test_results.append({
                'test': 'performance_benchmarks',
                'success': True,
                'details': benchmarks
            })
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.test_results.append({
                'test': 'performance_benchmarks',
                'success': False,
                'error': str(e)
            })
    
    def _print_test_summary(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½"""
        print("\n" + "=" * 80)
        print("ğŸ“Š í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        
        successful = sum(1 for t in self.test_results if t.get('success', False))
        total = len(self.test_results)
        
        print(f"ì„±ê³µ: {successful}/{total} ({successful/total*100:.1f}%)")
        print("\nìƒì„¸ ê²°ê³¼:")
        
        for result in self.test_results:
            status = "âœ…" if result.get('success') else "âŒ"
            print(f"{status} {result['test']}")
            
            if result.get('success') and 'details' in result:
                for key, value in result['details'].items():
                    print(f"   - {key}: {value}")
            elif 'error' in result:
                print(f"   - Error: {result['error']}")
                
    async def _generate_content_no_cache(self, topic: str):
        """ìºì‹œ ì—†ì´ ì½˜í…ì¸  ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
        papers = self._get_mock_papers()[:2]
        generator = ShortsScriptGenerator()
        return generator.generate(topic, papers, target_audience='general')
    
    def _get_mock_papers(self) -> List[Any]:
        """í…ŒìŠ¤íŠ¸ìš© ëª¨ì˜ ë…¼ë¬¸ ë°ì´í„°"""
        class MockPaper:
            def __init__(self, **kwargs):
                self.title = kwargs.get('title')
                self.authors = kwargs.get('authors')
                self.journal = kwargs.get('journal')
                self.year = kwargs.get('year')
                self.impact_factor = kwargs.get('impact_factor')
                self.citations = kwargs.get('citations', 0)
                self.paper_type = kwargs.get('paper_type', 'Original Research')
                self.doi = kwargs.get('doi', 'mock-doi')
                
        return [
            MockPaper(
                title="Effects of HIIT on Abdominal Fat Loss",
                authors="Smith J, Johnson K",
                journal="Sports Medicine",
                year=2024,
                impact_factor=11.2,
                citations=45,
                paper_type="Systematic Review"
            ),
            MockPaper(
                title="Protein Timing for Muscle Recovery",
                authors="Lee S, Park H",
                journal="Journal of Sports Science",
                year=2023,
                impact_factor=3.5,
                citations=22,
                paper_type="RCT"
            ),
            MockPaper(
                title="Core Training Methods Comparison",
                authors="Wilson R, Davis M",
                journal="Exercise Science Review",
                year=2023,
                impact_factor=5.8,
                citations=67,
                paper_type="Meta-analysis"
            )
        ]
    
    def _get_diverse_papers(self) -> List[Any]:
        """ë‹¤ì–‘í•œ í’ˆì§ˆì˜ ë…¼ë¬¸ ë°ì´í„°"""
        base_papers = self._get_mock_papers()
        
        # ë‹¤ì–‘í•œ í’ˆì§ˆì˜ ë…¼ë¬¸ ì¶”ê°€
        class MockPaper:
            def __init__(self, **kwargs):
                for key, value in kwargs.items():
                    setattr(self, key, value)
                    
        diverse_papers = base_papers + [
            MockPaper(
                title="Low Quality Study",
                authors="Unknown",
                journal="Predatory Journal",
                year=2020,
                impact_factor=0.5,
                citations=2,
                paper_type="Case Report"
            ),
            MockPaper(
                title="Medium Quality Research",
                authors="Brown A",
                journal="Regional Sports Journal",
                year=2022,
                impact_factor=2.1,
                citations=15,
                paper_type="Cross-sectional"
            )
        ]
        
        return diverse_papers


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜
async def run_integration_tests():
    """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    suite = IntegrationTestSuite()
    await suite.run_all_tests()

if __name__ == "__main__":
    asyncio.run(run_integration_tests())