"""
ë¶„ì„ ë° í†µê³„ API ì—”ë“œí¬ì¸íŠ¸
"""

from fastapi import APIRouter, HTTPException, Query
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from pydantic import BaseModel, Field
from collections import Counter, defaultdict

from ..services.advanced_cache_manager import advanced_cache
from ..services.performance_optimizer import performance_optimizer
from ..services.system_monitor import system_monitor
from ..utils.logging_config import app_logger

router = APIRouter()

# Pydantic ëª¨ë¸
class TimeRange(BaseModel):
    """ì‹œê°„ ë²”ìœ„"""
    start_date: Optional[datetime] = Field(None, description="ì‹œì‘ ë‚ ì§œ")
    end_date: Optional[datetime] = Field(None, description="ì¢…ë£Œ ë‚ ì§œ")
    last_hours: Optional[int] = Field(None, ge=1, le=168, description="ìµœê·¼ Nì‹œê°„")

class ContentAnalytics(BaseModel):
    """ì½˜í…ì¸  ìƒì„± ë¶„ì„"""
    total_generated: int
    by_type: Dict[str, int]
    by_quality: Dict[str, int]
    avg_quality_score: float
    avg_generation_time: float
    popular_topics: List[Dict[str, Any]]

class CategoryAnalytics(BaseModel):
    """ì¹´í…Œê³ ë¦¬ ë¶„ì„"""
    total_categories: int
    avg_practicality_score: float
    avg_interest_score: float
    by_practicality_range: Dict[str, int]
    most_popular: List[Dict[str, Any]]

class PaperAnalytics(BaseModel):
    """ë…¼ë¬¸ ë¶„ì„"""
    total_papers: int
    quality_distribution: Dict[str, int]
    avg_impact_factor: float
    avg_citations: int
    by_type: Dict[str, int]
    by_year: Dict[int, int]

class SystemAnalytics(BaseModel):
    """ì‹œìŠ¤í…œ ì‚¬ìš© ë¶„ì„"""
    api_calls: Dict[str, int]
    cache_performance: Dict[str, Any]
    error_rate: float
    avg_response_time: float
    peak_usage_hours: List[int]

@router.get("/overview")
async def get_analytics_overview(
    time_range: TimeRange = None
):
    """ì „ì²´ ë¶„ì„ ê°œìš”"""
    try:
        # ì‹œê°„ ë²”ìœ„ ì„¤ì •
        if time_range and time_range.last_hours:
            start_time = datetime.now() - timedelta(hours=time_range.last_hours)
            end_time = datetime.now()
        elif time_range and time_range.start_date and time_range.end_date:
            start_time = time_range.start_date
            end_time = time_range.end_date
        else:
            # ê¸°ë³¸ê°’: ìµœê·¼ 24ì‹œê°„
            start_time = datetime.now() - timedelta(hours=24)
            end_time = datetime.now()
        
        # ê° ë¶€ë¶„ ë¶„ì„ ìˆ˜ì§‘
        content_analytics = await _get_content_analytics(start_time, end_time)
        category_analytics = await _get_category_analytics(start_time, end_time)
        paper_analytics = await _get_paper_analytics(start_time, end_time)
        system_analytics = await _get_system_analytics(start_time, end_time)
        
        return {
            "time_range": {
                "start": start_time.isoformat(),
                "end": end_time.isoformat()
            },
            "content": content_analytics,
            "categories": category_analytics,
            "papers": paper_analytics,
            "system": system_analytics,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        app_logger.error(f"ë¶„ì„ ê°œìš” ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ë¶„ì„ ê°œìš” ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

@router.get("/content/metrics")
async def get_content_metrics(
    content_type: Optional[str] = Query(None, description="ì½˜í…ì¸  íƒ€ì… í•„í„°"),
    min_quality: Optional[float] = Query(None, ge=0, le=100, description="ìµœì†Œ í’ˆì§ˆ ì ìˆ˜")
):
    """ì½˜í…ì¸  ìƒì„± ë©”íŠ¸ë¦­"""
    try:
        # ìºì‹œì—ì„œ ì½˜í…ì¸  ê´€ë ¨ í•­ëª© ìˆ˜ì§‘
        content_entries = []
        
        for key, entry in advanced_cache.index.items():
            if entry.metadata and entry.metadata.get('content_type'):
                if content_type and entry.metadata['content_type'] != content_type:
                    continue
                if min_quality and entry.metadata.get('quality_score', 0) < min_quality:
                    continue
                    
                content_entries.append(entry)
        
        # í†µê³„ ê³„ì‚°
        total_count = len(content_entries)
        type_counts = Counter(e.metadata['content_type'] for e in content_entries)
        quality_scores = [e.metadata.get('quality_score', 0) for e in content_entries]
        
        # í’ˆì§ˆ ë²”ìœ„ë³„ ë¶„í¬
        quality_ranges = {
            '0-60': 0,
            '60-70': 0,
            '70-80': 0,
            '80-90': 0,
            '90-100': 0
        }
        
        for score in quality_scores:
            if score < 60:
                quality_ranges['0-60'] += 1
            elif score < 70:
                quality_ranges['60-70'] += 1
            elif score < 80:
                quality_ranges['70-80'] += 1
            elif score < 90:
                quality_ranges['80-90'] += 1
            else:
                quality_ranges['90-100'] += 1
        
        # ì¸ê¸° ì£¼ì œ (ìºì‹œ ì ‘ê·¼ íšŸìˆ˜ ê¸°ë°˜)
        topic_access = defaultdict(int)
        for entry in content_entries:
            topic = entry.metadata.get('topic', 'Unknown')
            topic_access[topic] += entry.access_count
            
        popular_topics = sorted(
            topic_access.items(),
            key=lambda x: x[1],
            reverse=True
        )[:10]
        
        return {
            "total_content": total_count,
            "by_type": dict(type_counts),
            "quality_distribution": quality_ranges,
            "avg_quality_score": sum(quality_scores) / len(quality_scores) if quality_scores else 0,
            "popular_topics": [
                {"topic": topic, "access_count": count}
                for topic, count in popular_topics
            ],
            "filters_applied": {
                "content_type": content_type,
                "min_quality": min_quality
            }
        }
        
    except Exception as e:
        app_logger.error(f"ì½˜í…ì¸  ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì½˜í…ì¸  ë©”íŠ¸ë¦­ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

@router.get("/categories/performance")
async def get_category_performance():
    """ì¹´í…Œê³ ë¦¬ ì„±ëŠ¥ ë¶„ì„"""
    try:
        # ëª¨ì˜ ë°ì´í„° (ì‹¤ì œë¡œëŠ” DBì—ì„œ ì¡°íšŒ)
        categories_data = [
            {"name": "ğŸ’ª í™ˆíŠ¸ë ˆì´ë‹", "practicality": 9.0, "interest": 8.5, "content_count": 45},
            {"name": "ğŸ¥— ê±´ê°•ì‹ë‹¨", "practicality": 8.5, "interest": 9.0, "content_count": 38},
            {"name": "ğŸ”¥ HIIT ìš´ë™", "practicality": 9.5, "interest": 9.8, "content_count": 52},
            {"name": "ğŸ§˜ ìš”ê°€/ëª…ìƒ", "practicality": 7.5, "interest": 8.0, "content_count": 25},
            {"name": "ğŸ’Š ë³´ì¶©ì œ ê°€ì´ë“œ", "practicality": 8.0, "interest": 7.5, "content_count": 30}
        ]
        
        # ì‹¤ìš©ì„±ë³„ ë¶„í¬
        practicality_ranges = {
            '6-7': 0,
            '7-8': 0,
            '8-9': 0,
            '9-10': 0
        }
        
        for cat in categories_data:
            score = cat['practicality']
            if score < 7:
                practicality_ranges['6-7'] += 1
            elif score < 8:
                practicality_ranges['7-8'] += 1
            elif score < 9:
                practicality_ranges['8-9'] += 1
            else:
                practicality_ranges['9-10'] += 1
        
        # ì½˜í…ì¸  ìƒì„±ëŸ‰ ê¸°ì¤€ ì •ë ¬
        top_categories = sorted(
            categories_data,
            key=lambda x: x['content_count'],
            reverse=True
        )[:5]
        
        return {
            "total_categories": len(categories_data),
            "avg_practicality_score": sum(c['practicality'] for c in categories_data) / len(categories_data),
            "avg_interest_score": sum(c['interest'] for c in categories_data) / len(categories_data),
            "practicality_distribution": practicality_ranges,
            "top_categories_by_content": top_categories,
            "correlation": {
                "practicality_vs_content": 0.82,  # ëª¨ì˜ ìƒê´€ê³„ìˆ˜
                "interest_vs_content": 0.75
            }
        }
        
    except Exception as e:
        app_logger.error(f"ì¹´í…Œê³ ë¦¬ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì¹´í…Œê³ ë¦¬ ì„±ëŠ¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

@router.get("/papers/quality")
async def get_paper_quality_analytics():
    """ë…¼ë¬¸ í’ˆì§ˆ ë¶„ì„"""
    try:
        # ëª¨ì˜ ë°ì´í„°
        papers_data = {
            "total_papers": 150,
            "quality_distribution": {
                "A+": 12,
                "A": 28,
                "B+": 45,
                "B": 35,
                "C": 20,
                "D": 10
            },
            "by_type": {
                "Systematic Review": 15,
                "Meta-analysis": 18,
                "RCT": 42,
                "Cohort Study": 30,
                "Cross-sectional": 25,
                "Case Report": 20
            },
            "impact_factor_ranges": {
                "0-2": 30,
                "2-5": 45,
                "5-10": 50,
                "10+": 25
            },
            "citation_ranges": {
                "0-10": 40,
                "10-50": 55,
                "50-100": 35,
                "100+": 20
            }
        }
        
        # ê³ í’ˆì§ˆ ë…¼ë¬¸ ë¹„ìœ¨
        high_quality = (
            papers_data["quality_distribution"]["A+"] +
            papers_data["quality_distribution"]["A"] +
            papers_data["quality_distribution"]["B+"]
        )
        high_quality_ratio = high_quality / papers_data["total_papers"] * 100
        
        return {
            **papers_data,
            "high_quality_ratio": high_quality_ratio,
            "avg_quality_score": 72.5,  # ëª¨ì˜ í‰ê· 
            "trends": {
                "quality_improving": True,
                "avg_if_trend": "+2.3% monthly",
                "citation_trend": "+5.1% monthly"
            }
        }
        
    except Exception as e:
        app_logger.error(f"ë…¼ë¬¸ í’ˆì§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ë…¼ë¬¸ í’ˆì§ˆ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

@router.get("/system/usage")
async def get_system_usage():
    """ì‹œìŠ¤í…œ ì‚¬ìš© í†µê³„"""
    try:
        # ì„±ëŠ¥ ë¦¬í¬íŠ¸
        perf_report = performance_optimizer.get_performance_report()
        
        # ìºì‹œ í†µê³„
        cache_stats = advanced_cache.get_stats()
        
        # API í˜¸ì¶œ í†µê³„ (ëª¨ì˜)
        api_calls = {
            "categories": 1250,
            "papers": 890,
            "contents": 2100,
            "cache": 450,
            "health": 3200
        }
        
        # ì‹œê°„ëŒ€ë³„ ì‚¬ìš©ëŸ‰ (ëª¨ì˜)
        hourly_usage = {}
        for hour in range(24):
            if 7 <= hour <= 10 or 18 <= hour <= 21:  # í”¼í¬ ì‹œê°„
                hourly_usage[hour] = 150 + (hour % 3) * 20
            else:
                hourly_usage[hour] = 50 + (hour % 5) * 10
        
        # í”¼í¬ ì‹œê°„ ì°¾ê¸°
        peak_hours = sorted(
            hourly_usage.items(),
            key=lambda x: x[1],
            reverse=True
        )[:3]
        
        return {
            "api_calls": {
                "total": sum(api_calls.values()),
                "by_endpoint": api_calls,
                "avg_per_hour": sum(api_calls.values()) / 24
            },
            "performance": {
                "avg_response_time_ms": perf_report.get('performance', {}).get('avg_duration', 0),
                "success_rate": perf_report.get('success_rate', 0),
                "error_rate": 100 - perf_report.get('success_rate', 100)
            },
            "cache": {
                "hit_rate": (cache_stats.hit_count / (cache_stats.hit_count + cache_stats.miss_count) * 100) 
                           if (cache_stats.hit_count + cache_stats.miss_count) > 0 else 0,
                "total_entries": cache_stats.total_entries,
                "size_mb": cache_stats.total_size_mb
            },
            "usage_patterns": {
                "hourly_distribution": hourly_usage,
                "peak_hours": [hour for hour, _ in peak_hours],
                "busiest_day": "Wednesday",  # ëª¨ì˜
                "avg_daily_requests": sum(api_calls.values()) / 7
            }
        }
        
    except Exception as e:
        app_logger.error(f"ì‹œìŠ¤í…œ ì‚¬ìš© í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ì‹œìŠ¤í…œ ì‚¬ìš© í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

@router.get("/trends")
async def get_analytics_trends(
    metric: str = Query(..., description="ë¶„ì„í•  ë©”íŠ¸ë¦­ (content_quality, category_popularity, paper_quality, api_usage)"),
    period: str = Query("week", regex="^(day|week|month)$", description="ê¸°ê°„")
):
    """íŠ¸ë Œë“œ ë¶„ì„"""
    try:
        # ê¸°ê°„ë³„ ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜
        data_points = {
            "day": 24,     # ì‹œê°„ë³„
            "week": 7,     # ì¼ë³„
            "month": 30    # ì¼ë³„
        }
        
        points = data_points[period]
        
        # ëª¨ì˜ íŠ¸ë Œë“œ ë°ì´í„° ìƒì„±
        if metric == "content_quality":
            trend_data = [
                {"time": i, "value": 70 + (i % 5) * 2 + (i / points) * 5}
                for i in range(points)
            ]
        elif metric == "category_popularity":
            trend_data = [
                {"time": i, "value": 100 + (i % 7) * 10 - (i % 3) * 5}
                for i in range(points)
            ]
        elif metric == "paper_quality":
            trend_data = [
                {"time": i, "value": 65 + (i % 4) * 3 + (i / points) * 3}
                for i in range(points)
            ]
        elif metric == "api_usage":
            trend_data = [
                {"time": i, "value": 200 + (i % 6) * 30 + (i % 2) * 50}
                for i in range(points)
            ]
        else:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤")
        
        # íŠ¸ë Œë“œ ê³„ì‚°
        values = [d["value"] for d in trend_data]
        avg_value = sum(values) / len(values)
        trend_direction = "increasing" if values[-1] > values[0] else "decreasing"
        change_percent = ((values[-1] - values[0]) / values[0]) * 100
        
        return {
            "metric": metric,
            "period": period,
            "data": trend_data,
            "summary": {
                "current_value": values[-1],
                "avg_value": avg_value,
                "min_value": min(values),
                "max_value": max(values),
                "trend": trend_direction,
                "change_percent": change_percent
            },
            "generated_at": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        app_logger.error(f"íŠ¸ë Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="íŠ¸ë Œë“œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

@router.post("/export")
async def export_analytics(
    format: str = Query("json", regex="^(json|csv)$", description="ë‚´ë³´ë‚´ê¸° í˜•ì‹"),
    include_raw_data: bool = Query(False, description="ì›ì‹œ ë°ì´í„° í¬í•¨ ì—¬ë¶€")
):
    """ë¶„ì„ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"""
    try:
        # ì „ì²´ ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘
        analytics_data = {
            "export_info": {
                "timestamp": datetime.now().isoformat(),
                "format": format,
                "include_raw_data": include_raw_data
            },
            "overview": await get_analytics_overview(),
            "content_metrics": await get_content_metrics(),
            "category_performance": await get_category_performance(),
            "paper_quality": await get_paper_quality_analytics(),
            "system_usage": await get_system_usage()
        }
        
        # íŒŒì¼ ì €ì¥
        export_dir = "./exports/analytics"
        os.makedirs(export_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format == "json":
            import json
            export_path = f"{export_dir}/analytics_{timestamp}.json"
            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(analytics_data, f, indent=2, ensure_ascii=False)
        else:  # csv
            import csv
            export_path = f"{export_dir}/analytics_{timestamp}.csv"
            # CSVëŠ” í”Œë« êµ¬ì¡°ë§Œ ì§€ì›í•˜ë¯€ë¡œ ìš”ì•½ ì •ë³´ë§Œ ë‚´ë³´ë‚´ê¸°
            with open(export_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(["Metric", "Value"])
                writer.writerow(["Export Time", analytics_data["export_info"]["timestamp"]])
                writer.writerow(["Total Content", analytics_data["content_metrics"]["total_content"]])
                writer.writerow(["Avg Quality Score", analytics_data["content_metrics"]["avg_quality_score"]])
                writer.writerow(["Total Categories", analytics_data["category_performance"]["total_categories"]])
                writer.writerow(["Cache Hit Rate", analytics_data["system_usage"]["cache"]["hit_rate"]])
        
        return {
            "status": "success",
            "export_path": export_path,
            "format": format,
            "file_size_kb": os.path.getsize(export_path) / 1024,
            "message": "ë¶„ì„ ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë‚´ë³´ë‚´ì¡ŒìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        app_logger.error(f"ë¶„ì„ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="ë¶„ì„ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")

# ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤
async def _get_content_analytics(start_time: datetime, end_time: datetime) -> Dict[str, Any]:
    """ì½˜í…ì¸  ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘"""
    # ì‹¤ì œë¡œëŠ” DBì—ì„œ ì‹œê°„ ë²”ìœ„ì— ë§ëŠ” ë°ì´í„° ì¡°íšŒ
    return {
        "total_generated": 125,
        "by_type": {"shorts": 45, "article": 50, "report": 30},
        "by_quality": {"excellent": 35, "good": 60, "average": 30},
        "avg_quality_score": 78.5,
        "avg_generation_time": 2.3,
        "popular_topics": [
            {"topic": "HIIT ìš´ë™", "count": 15},
            {"topic": "ë‹¨ë°±ì§ˆ ì„­ì·¨", "count": 12},
            {"topic": "í™ˆíŠ¸ë ˆì´ë‹", "count": 10}
        ]
    }

async def _get_category_analytics(start_time: datetime, end_time: datetime) -> Dict[str, Any]:
    """ì¹´í…Œê³ ë¦¬ ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘"""
    return {
        "total_categories": 45,
        "avg_practicality_score": 8.2,
        "avg_interest_score": 8.5,
        "by_practicality_range": {"6-7": 5, "7-8": 10, "8-9": 20, "9-10": 10},
        "most_popular": [
            {"name": "í™ˆíŠ¸ë ˆì´ë‹", "usage_count": 250},
            {"name": "ë‹¤ì´ì–´íŠ¸", "usage_count": 180}
        ]
    }

async def _get_paper_analytics(start_time: datetime, end_time: datetime) -> Dict[str, Any]:
    """ë…¼ë¬¸ ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘"""
    return {
        "total_papers": 89,
        "quality_distribution": {"A+": 10, "A": 20, "B+": 30, "B": 20, "C": 9},
        "avg_impact_factor": 6.8,
        "avg_citations": 45,
        "by_type": {"Systematic Review": 10, "RCT": 25, "Meta-analysis": 15, "Other": 39},
        "by_year": {2024: 25, 2023: 35, 2022: 20, 2021: 9}
    }

async def _get_system_analytics(start_time: datetime, end_time: datetime) -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘"""
    return {
        "api_calls": {"total": 5800, "by_endpoint": {"contents": 2100, "categories": 1500, "papers": 1200, "other": 1000}},
        "cache_performance": {"hit_rate": 85.5, "total_size_mb": 450},
        "error_rate": 0.5,
        "avg_response_time": 125,
        "peak_usage_hours": [9, 14, 20]
    }