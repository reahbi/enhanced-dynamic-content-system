"""
ê³ ê¸‰ ìºì‹œ ë§¤ë‹ˆì € - í–¥ìƒëœ íŒŒì¼ ê¸°ë°˜ ìºì‹± ì‹œìŠ¤í…œ
"""

import os
import json
import pickle
import hashlib
import time
import asyncio
import aiofiles
from typing import Any, Optional, Dict, List, Tuple, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import threading
from pathlib import Path
import shutil
import gzip

@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬"""
    key: str
    value: Any
    created_at: float
    expires_at: Optional[float]
    access_count: int = 0
    last_accessed: float = None
    size_bytes: int = 0
    compression: bool = False
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.last_accessed is None:
            self.last_accessed = self.created_at

@dataclass
class CacheStats:
    """ìºì‹œ í†µê³„"""
    total_entries: int
    total_size_mb: float
    hit_count: int
    miss_count: int
    eviction_count: int
    compression_ratio: float
    avg_access_time_ms: float
    most_accessed_keys: List[Tuple[str, int]]

class AdvancedCacheManager:
    """ê³ ê¸‰ ìºì‹œ ë§¤ë‹ˆì €"""
    
    def __init__(self, 
                 cache_dir: str = "./cache/advanced",
                 max_size_mb: float = 1024,  # 1GB
                 default_ttl: int = 3600,
                 enable_compression: bool = True,
                 enable_async: bool = True):
        
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        self.max_size_mb = max_size_mb
        self.default_ttl = default_ttl
        self.enable_compression = enable_compression
        self.enable_async = enable_async
        
        # ì¸ë±ìŠ¤ íŒŒì¼
        self.index_file = self.cache_dir / "cache_index.json"
        self.stats_file = self.cache_dir / "cache_stats.json"
        
        # ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤
        self.index: Dict[str, CacheEntry] = {}
        self.stats = {
            'hit_count': 0,
            'miss_count': 0,
            'eviction_count': 0,
            'total_access_time': 0,
            'access_count': 0
        }
        
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±
        self._lock = threading.Lock()
        self._async_lock = asyncio.Lock()
        
        # ì´ˆê¸°í™”
        self._load_index()
        self._start_maintenance_tasks()
        
    def _load_index(self):
        """ì¸ë±ìŠ¤ ë¡œë“œ"""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r') as f:
                    index_data = json.load(f)
                    
                for key, entry_data in index_data.items():
                    self.index[key] = CacheEntry(**entry_data)
                    
            except Exception as e:
                print(f"ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                
    def _save_index(self):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            index_data = {}
            for key, entry in self.index.items():
                entry_dict = asdict(entry)
                # ì§ë ¬í™” ë¶ˆê°€ëŠ¥í•œ ê°’ ì œê±°
                entry_dict['value'] = None
                index_data[key] = entry_dict
                
            with open(self.index_file, 'w') as f:
                json.dump(index_data, f)
                
        except Exception as e:
            print(f"ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            
    def _get_cache_path(self, key: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        # í‚¤ë¥¼ í•´ì‹œí•˜ì—¬ íŒŒì¼ëª… ìƒì„±
        key_hash = hashlib.md5(key.encode()).hexdigest()
        
        # ë””ë ‰í† ë¦¬ ë¶„ì‚° (ì²« 2ê¸€ìë¡œ ì„œë¸Œë””ë ‰í† ë¦¬)
        subdir = self.cache_dir / key_hash[:2]
        subdir.mkdir(exist_ok=True)
        
        return subdir / f"{key_hash}.cache"
        
    def _serialize(self, value: Any, compress: bool = False) -> bytes:
        """ê°’ ì§ë ¬í™”"""
        data = pickle.dumps(value)
        
        if compress and self.enable_compression:
            data = gzip.compress(data)
            
        return data
        
    def _deserialize(self, data: bytes, compressed: bool = False) -> Any:
        """ê°’ ì—­ì§ë ¬í™”"""
        if compressed and self.enable_compression:
            data = gzip.decompress(data)
            
        return pickle.loads(data)
        
    def set(self, key: str, value: Any, ttl: Optional[int] = None, 
            metadata: Optional[Dict[str, Any]] = None) -> bool:
        """ìºì‹œ ì„¤ì •"""
        start_time = time.time()
        
        try:
            with self._lock:
                # TTL ì„¤ì •
                if ttl is None:
                    ttl = self.default_ttl
                    
                expires_at = time.time() + ttl if ttl > 0 else None
                
                # ì••ì¶• ì—¬ë¶€ ê²°ì • (í° ë°ì´í„°ë§Œ ì••ì¶•)
                data = self._serialize(value)
                compress = len(data) > 1024 * 10  # 10KB ì´ìƒ
                
                if compress:
                    compressed_data = self._serialize(value, compress=True)
                    compression_ratio = len(compressed_data) / len(data)
                    
                    # ì••ì¶• íš¨ìœ¨ì´ ì¢‹ì„ ë•Œë§Œ ì‚¬ìš©
                    if compression_ratio < 0.9:
                        data = compressed_data
                    else:
                        compress = False
                        
                # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                entry = CacheEntry(
                    key=key,
                    value=None,  # íŒŒì¼ì— ì €ì¥
                    created_at=time.time(),
                    expires_at=expires_at,
                    size_bytes=len(data),
                    compression=compress,
                    metadata=metadata
                )
                
                # í¬ê¸° í™•ì¸ ë° ì •ë¦¬
                self._ensure_cache_size()
                
                # íŒŒì¼ ì €ì¥
                cache_path = self._get_cache_path(key)
                with open(cache_path, 'wb') as f:
                    f.write(data)
                    
                # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
                self.index[key] = entry
                self._save_index()
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                access_time = (time.time() - start_time) * 1000
                self._update_stats('set', access_time)
                
                return True
                
        except Exception as e:
            print(f"ìºì‹œ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
            
    def get(self, key: str, default: Any = None) -> Any:
        """ìºì‹œ ì¡°íšŒ"""
        start_time = time.time()
        
        try:
            with self._lock:
                # ì¸ë±ìŠ¤ í™•ì¸
                if key not in self.index:
                    self.stats['miss_count'] += 1
                    return default
                    
                entry = self.index[key]
                
                # ë§Œë£Œ í™•ì¸
                if entry.expires_at and time.time() > entry.expires_at:
                    self._remove_entry(key)
                    self.stats['miss_count'] += 1
                    return default
                    
                # íŒŒì¼ ì½ê¸°
                cache_path = self._get_cache_path(key)
                if not cache_path.exists():
                    self._remove_entry(key)
                    self.stats['miss_count'] += 1
                    return default
                    
                with open(cache_path, 'rb') as f:
                    data = f.read()
                    
                # ì—­ì§ë ¬í™”
                value = self._deserialize(data, compressed=entry.compression)
                
                # ì ‘ê·¼ ì •ë³´ ì—…ë°ì´íŠ¸
                entry.access_count += 1
                entry.last_accessed = time.time()
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.stats['hit_count'] += 1
                access_time = (time.time() - start_time) * 1000
                self._update_stats('get', access_time)
                
                return value
                
        except Exception as e:
            print(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            self.stats['miss_count'] += 1
            return default
            
    async def aset(self, key: str, value: Any, ttl: Optional[int] = None,
                   metadata: Optional[Dict[str, Any]] = None) -> bool:
        """ë¹„ë™ê¸° ìºì‹œ ì„¤ì •"""
        if not self.enable_async:
            return self.set(key, value, ttl, metadata)
            
        start_time = time.time()
        
        try:
            async with self._async_lock:
                # TTL ì„¤ì •
                if ttl is None:
                    ttl = self.default_ttl
                    
                expires_at = time.time() + ttl if ttl > 0 else None
                
                # ì§ë ¬í™” (CPU ì§‘ì•½ì ì´ë¯€ë¡œ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰)
                loop = asyncio.get_event_loop()
                data = await loop.run_in_executor(None, self._serialize, value)
                
                compress = len(data) > 1024 * 10
                if compress:
                    compressed_data = await loop.run_in_executor(
                        None, self._serialize, value, True
                    )
                    if len(compressed_data) < len(data) * 0.9:
                        data = compressed_data
                    else:
                        compress = False
                        
                # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                entry = CacheEntry(
                    key=key,
                    value=None,
                    created_at=time.time(),
                    expires_at=expires_at,
                    size_bytes=len(data),
                    compression=compress,
                    metadata=metadata
                )
                
                # í¬ê¸° í™•ì¸
                await loop.run_in_executor(None, self._ensure_cache_size)
                
                # íŒŒì¼ ì €ì¥
                cache_path = self._get_cache_path(key)
                async with aiofiles.open(cache_path, 'wb') as f:
                    await f.write(data)
                    
                # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
                self.index[key] = entry
                await loop.run_in_executor(None, self._save_index)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                access_time = (time.time() - start_time) * 1000
                self._update_stats('aset', access_time)
                
                return True
                
        except Exception as e:
            print(f"ë¹„ë™ê¸° ìºì‹œ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
            
    async def aget(self, key: str, default: Any = None) -> Any:
        """ë¹„ë™ê¸° ìºì‹œ ì¡°íšŒ"""
        if not self.enable_async:
            return self.get(key, default)
            
        start_time = time.time()
        
        try:
            async with self._async_lock:
                # ì¸ë±ìŠ¤ í™•ì¸
                if key not in self.index:
                    self.stats['miss_count'] += 1
                    return default
                    
                entry = self.index[key]
                
                # ë§Œë£Œ í™•ì¸
                if entry.expires_at and time.time() > entry.expires_at:
                    await self._aremove_entry(key)
                    self.stats['miss_count'] += 1
                    return default
                    
                # íŒŒì¼ ì½ê¸°
                cache_path = self._get_cache_path(key)
                if not cache_path.exists():
                    await self._aremove_entry(key)
                    self.stats['miss_count'] += 1
                    return default
                    
                async with aiofiles.open(cache_path, 'rb') as f:
                    data = await f.read()
                    
                # ì—­ì§ë ¬í™”
                loop = asyncio.get_event_loop()
                value = await loop.run_in_executor(
                    None, self._deserialize, data, entry.compression
                )
                
                # ì ‘ê·¼ ì •ë³´ ì—…ë°ì´íŠ¸
                entry.access_count += 1
                entry.last_accessed = time.time()
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.stats['hit_count'] += 1
                access_time = (time.time() - start_time) * 1000
                self._update_stats('aget', access_time)
                
                return value
                
        except Exception as e:
            print(f"ë¹„ë™ê¸° ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            self.stats['miss_count'] += 1
            return default
            
    def delete(self, key: str) -> bool:
        """ìºì‹œ ì‚­ì œ"""
        with self._lock:
            return self._remove_entry(key)
            
    def clear(self) -> int:
        """ì „ì²´ ìºì‹œ ì‚­ì œ"""
        with self._lock:
            count = len(self.index)
            
            # ìºì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ ë° ì¬ìƒì„±
            shutil.rmtree(self.cache_dir, ignore_errors=True)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            
            # ì¸ë±ìŠ¤ ì´ˆê¸°í™”
            self.index.clear()
            self._save_index()
            
            return count
            
    def _remove_entry(self, key: str) -> bool:
        """ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°"""
        if key not in self.index:
            return False
            
        try:
            # íŒŒì¼ ì‚­ì œ
            cache_path = self._get_cache_path(key)
            if cache_path.exists():
                cache_path.unlink()
                
            # ì¸ë±ìŠ¤ì—ì„œ ì œê±°
            del self.index[key]
            
            return True
            
        except Exception as e:
            print(f"ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±° ì‹¤íŒ¨: {e}")
            return False
            
    async def _aremove_entry(self, key: str) -> bool:
        """ë¹„ë™ê¸° ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°"""
        if key not in self.index:
            return False
            
        try:
            cache_path = self._get_cache_path(key)
            if cache_path.exists():
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(None, cache_path.unlink)
                
            del self.index[key]
            return True
            
        except Exception as e:
            print(f"ë¹„ë™ê¸° ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±° ì‹¤íŒ¨: {e}")
            return False
            
    def _ensure_cache_size(self):
        """ìºì‹œ í¬ê¸° ê´€ë¦¬"""
        total_size = sum(entry.size_bytes for entry in self.index.values())
        max_size_bytes = self.max_size_mb * 1024 * 1024
        
        if total_size <= max_size_bytes:
            return
            
        # LRU ë°©ì‹ìœ¼ë¡œ ì •ë¦¬
        entries = sorted(
            self.index.items(),
            key=lambda x: x[1].last_accessed
        )
        
        while total_size > max_size_bytes * 0.9:  # 90%ê¹Œì§€ ì •ë¦¬
            if not entries:
                break
                
            key, entry = entries.pop(0)
            if self._remove_entry(key):
                total_size -= entry.size_bytes
                self.stats['eviction_count'] += 1
                
    def _update_stats(self, operation: str, access_time_ms: float):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats['total_access_time'] += access_time_ms
        self.stats['access_count'] += 1
        
    def get_stats(self) -> CacheStats:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        with self._lock:
            # ì „ì²´ í¬ê¸° ê³„ì‚°
            total_size_bytes = sum(entry.size_bytes for entry in self.index.values())
            total_size_mb = total_size_bytes / 1024 / 1024
            
            # ì••ì¶•ë¥  ê³„ì‚°
            compressed_entries = [e for e in self.index.values() if e.compression]
            compression_ratio = len(compressed_entries) / max(len(self.index), 1)
            
            # í‰ê·  ì ‘ê·¼ ì‹œê°„
            avg_access_time = (
                self.stats['total_access_time'] / max(self.stats['access_count'], 1)
            )
            
            # ê°€ì¥ ë§ì´ ì ‘ê·¼ëœ í‚¤
            most_accessed = sorted(
                [(k, e.access_count) for k, e in self.index.items()],
                key=lambda x: x[1],
                reverse=True
            )[:10]
            
            return CacheStats(
                total_entries=len(self.index),
                total_size_mb=total_size_mb,
                hit_count=self.stats['hit_count'],
                miss_count=self.stats['miss_count'],
                eviction_count=self.stats['eviction_count'],
                compression_ratio=compression_ratio,
                avg_access_time_ms=avg_access_time,
                most_accessed_keys=most_accessed
            )
            
    def _start_maintenance_tasks(self):
        """ìœ ì§€ë³´ìˆ˜ ì‘ì—… ì‹œì‘"""
        # ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì •ë¦¬ ì‘ì—…
        def cleanup_expired():
            while True:
                time.sleep(300)  # 5ë¶„ë§ˆë‹¤
                with self._lock:
                    expired_keys = []
                    for key, entry in self.index.items():
                        if entry.expires_at and time.time() > entry.expires_at:
                            expired_keys.append(key)
                            
                    for key in expired_keys:
                        self._remove_entry(key)
                        
                    if expired_keys:
                        self._save_index()
                        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰
        cleanup_thread = threading.Thread(target=cleanup_expired, daemon=True)
        cleanup_thread.start()
        
    def export_stats(self, filepath: str):
        """í†µê³„ ë‚´ë³´ë‚´ê¸°"""
        stats = self.get_stats()
        stats_dict = asdict(stats)
        stats_dict['export_time'] = datetime.now().isoformat()
        
        with open(filepath, 'w') as f:
            json.dump(stats_dict, f, indent=2)
            
        print(f"ğŸ“Š ìºì‹œ í†µê³„ ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filepath}")


# ì „ì—­ ê³ ê¸‰ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
advanced_cache = AdvancedCacheManager()