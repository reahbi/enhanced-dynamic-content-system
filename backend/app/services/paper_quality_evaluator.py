"""
ë…¼ë¬¸ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ - í•™ìˆ  ë…¼ë¬¸ì˜ ì‹ ë¢°ì„±ê³¼ ì˜í–¥ë ¥ì„ ìë™ í‰ê°€
"""

from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
import re

class QualityGrade(str, Enum):
    """ë…¼ë¬¸ í’ˆì§ˆ ë“±ê¸‰"""
    A_PLUS = "A+"
    A = "A"
    B_PLUS = "B+"
    B = "B"
    C = "C"
    D = "D"

@dataclass
class QualityMetrics:
    """ë…¼ë¬¸ í’ˆì§ˆ ë©”íŠ¸ë¦­"""
    paper_type_score: float  # ë…¼ë¬¸ ìœ í˜• ì ìˆ˜ (ìµœëŒ€ 35ì )
    impact_factor_score: float  # Impact Factor ì ìˆ˜ (ìµœëŒ€ 30ì )
    citation_score: float  # ì¸ìš© ìˆ˜ ì ìˆ˜ (ìµœëŒ€ 20ì )
    recency_score: float  # ìµœì‹ ì„± ì ìˆ˜ (ìµœëŒ€ 15ì )
    total_score: float  # ì´ì  (ìµœëŒ€ 100ì )
    quality_grade: QualityGrade  # ë“±ê¸‰ (A+, A, B+, B, C)
    
@dataclass
class PaperInfo:
    """ë…¼ë¬¸ ì •ë³´"""
    title: str
    authors: str
    journal: str
    year: int
    doi: str
    impact_factor: float
    citations: int
    paper_type: str

@dataclass
class QualityInfo:
    """ë…¼ë¬¸ í’ˆì§ˆ í‰ê°€ ê²°ê³¼"""
    quality_score: float
    grade: QualityGrade
    details: Dict[str, float]
    strengths: List[str]
    weaknesses: List[str]

class PaperQualityEvaluator:
    """ë…¼ë¬¸ í’ˆì§ˆ í‰ê°€ ì—”ì§„"""
    
    def __init__(self):
        # ë…¼ë¬¸ ìœ í˜•ë³„ ê¸°ë³¸ ì ìˆ˜
        self.paper_type_scores = {
            "Systematic Review": 35,
            "Meta-analysis": 35,
            "Systematic Review & Meta-analysis": 35,
            "Randomized Controlled Trial": 30,
            "RCT": 30,
            "Cohort Study": 25,
            "Case-Control Study": 20,
            "Cross-sectional Study": 15,
            "Case Report": 10,
            "Review": 20,
            "Research Article": 15,
            "Original Article": 15,
            "Clinical Trial": 25,
            "Longitudinal Study": 25,
            "Prospective Study": 25,
            "Retrospective Study": 20,
            "Observational Study": 15,
            "Pilot Study": 10,
            "Editorial": 5,
            "Letter": 5,
            "Commentary": 5
        }
        
        # ì €ë„ Impact Factor ë²”ìœ„
        self.impact_factor_ranges = {
            "top_tier": 10.0,  # Nature, Science ë“±
            "high": 5.0,       # ì£¼ìš” ì „ë¬¸ ì €ë„
            "medium": 3.0,     # ì¼ë°˜ ì „ë¬¸ ì €ë„
            "low": 1.0         # ê¸°íƒ€ ì €ë„
        }
        
        # í˜„ì¬ ì—°ë„
        self.current_year = datetime.now().year
    
    def evaluate_paper_metrics(self, paper: PaperInfo) -> QualityMetrics:
        """ë…¼ë¬¸ í’ˆì§ˆ ì¢…í•© í‰ê°€"""
        
        # 1. ë…¼ë¬¸ ìœ í˜• ì ìˆ˜ ê³„ì‚°
        paper_type_score = self._calculate_paper_type_score(paper.paper_type)
        
        # 2. Impact Factor ì ìˆ˜ ê³„ì‚°
        impact_factor_score = self._calculate_impact_factor_score(paper.impact_factor)
        
        # 3. ì¸ìš© ìˆ˜ ì ìˆ˜ ê³„ì‚°
        citation_score = self._calculate_citation_score(paper.citations, paper.year)
        
        # 4. ìµœì‹ ì„± ì ìˆ˜ ê³„ì‚°
        recency_score = self._calculate_recency_score(paper.year)
        
        # ì´ì  ê³„ì‚°
        total_score = (
            paper_type_score + 
            impact_factor_score + 
            citation_score + 
            recency_score
        )
        
        # ë“±ê¸‰ í• ë‹¹
        quality_grade = self._assign_quality_grade(total_score)
        
        return QualityMetrics(
            paper_type_score=paper_type_score,
            impact_factor_score=impact_factor_score,
            citation_score=citation_score,
            recency_score=recency_score,
            total_score=total_score,
            quality_grade=quality_grade
        )
    
    def evaluate_paper(self, paper) -> QualityInfo:
        """ë…¼ë¬¸ í’ˆì§ˆ í‰ê°€ (APIìš©)"""
        # PaperInfo í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í•„ìš”í•œ ê²½ìš°)
        if not isinstance(paper, PaperInfo):
            paper_info = PaperInfo(
                title=getattr(paper, 'title', ''),
                authors=getattr(paper, 'authors', ''),
                journal=getattr(paper, 'journal', ''),
                year=getattr(paper, 'year', datetime.now().year),
                doi=getattr(paper, 'doi', ''),
                impact_factor=getattr(paper, 'impact_factor', 0.0),
                citations=getattr(paper, 'citations', 0),
                paper_type=getattr(paper, 'paper_type', 'Research Article')
            )
        else:
            paper_info = paper
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = self.evaluate_paper_metrics(paper_info)
        
        # ìƒì„¸ ì •ë³´
        details = {
            'paper_type_score': metrics.paper_type_score,
            'impact_factor_score': metrics.impact_factor_score,
            'citation_score': metrics.citation_score,
            'recency_score': metrics.recency_score
        }
        
        # ê°•ì ê³¼ ì•½ì  ë¶„ì„
        strengths = []
        weaknesses = []
        
        # ë…¼ë¬¸ ìœ í˜• í‰ê°€
        if metrics.paper_type_score >= 30:
            strengths.append("ë†’ì€ ì¦ê±° ìˆ˜ì¤€ì˜ ì—°êµ¬ ì„¤ê³„")
        elif metrics.paper_type_score <= 15:
            weaknesses.append("ë‚®ì€ ì¦ê±° ìˆ˜ì¤€ì˜ ì—°êµ¬ ì„¤ê³„")
        
        # Impact Factor í‰ê°€
        if metrics.impact_factor_score >= 20:
            strengths.append("ì˜í–¥ë ¥ ë†’ì€ ì €ë„ì— ê²Œì¬")
        elif metrics.impact_factor_score <= 10:
            weaknesses.append("ë‚®ì€ ì˜í–¥ë ¥ì˜ ì €ë„")
        
        # ì¸ìš© ìˆ˜ í‰ê°€
        if metrics.citation_score >= 15:
            strengths.append("ë†’ì€ ì¸ìš© ìˆ˜")
        elif metrics.citation_score <= 5:
            weaknesses.append("ë‚®ì€ ì¸ìš© ìˆ˜")
        
        # ìµœì‹ ì„± í‰ê°€
        if metrics.recency_score >= 12:
            strengths.append("ìµœì‹  ì—°êµ¬")
        elif metrics.recency_score <= 3:
            weaknesses.append("ì˜¤ë˜ëœ ì—°êµ¬")
        
        return QualityInfo(
            quality_score=metrics.total_score,
            grade=metrics.quality_grade,
            details=details,
            strengths=strengths,
            weaknesses=weaknesses
        )
    
    def _calculate_paper_type_score(self, paper_type: str) -> float:
        """ë…¼ë¬¸ ìœ í˜•ë³„ ì ìˆ˜ ê³„ì‚° (ìµœëŒ€ 35ì )"""
        
        # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
        for type_key, score in self.paper_type_scores.items():
            if paper_type.lower() == type_key.lower():
                return float(score)
        
        # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
        paper_type_lower = paper_type.lower()
        for type_key, score in self.paper_type_scores.items():
            if type_key.lower() in paper_type_lower or paper_type_lower in type_key.lower():
                return float(score)
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­
        if "systematic" in paper_type_lower and "review" in paper_type_lower:
            return 35.0
        elif "meta" in paper_type_lower and "analysis" in paper_type_lower:
            return 35.0
        elif "randomized" in paper_type_lower or "rct" in paper_type_lower:
            return 30.0
        elif "cohort" in paper_type_lower:
            return 25.0
        elif "trial" in paper_type_lower:
            return 25.0
        elif "review" in paper_type_lower:
            return 20.0
        elif "study" in paper_type_lower:
            return 15.0
        else:
            return 10.0  # ê¸°ë³¸ê°’
    
    def _calculate_impact_factor_score(self, impact_factor: float) -> float:
        """Impact Factor ì ìˆ˜ ê³„ì‚° (ìµœëŒ€ 30ì )"""
        
        # IF Ã— 2, ìµœëŒ€ 30ì 
        score = impact_factor * 2
        
        # ìƒí•œì„  ì ìš©
        return min(score, 30.0)
    
    def _calculate_citation_score(self, citations: int, year: int) -> float:
        """ì¸ìš© ìˆ˜ ì ìˆ˜ ê³„ì‚° (ìµœëŒ€ 20ì )"""
        
        # ì—°ê°„ ì¸ìš© ìˆ˜ ê³„ì‚° (ë…¼ë¬¸ ì—°ë ¹ ê³ ë ¤)
        years_since_publication = max(1, self.current_year - year)
        annual_citations = citations / years_since_publication
        
        # ì—°ê°„ ì¸ìš© ìˆ˜ Ã— 2, ìµœëŒ€ 20ì 
        score = annual_citations * 2
        
        # ìƒí•œì„  ì ìš©
        return min(score, 20.0)
    
    def _calculate_recency_score(self, year: int) -> float:
        """ìµœì‹ ì„± ì ìˆ˜ ê³„ì‚° (ìµœëŒ€ 15ì )"""
        
        years_old = self.current_year - year
        
        if years_old <= 1:
            return 15.0
        elif years_old <= 2:
            return 12.0
        elif years_old <= 3:
            return 9.0
        elif years_old <= 5:
            return 6.0
        elif years_old <= 7:
            return 3.0
        else:
            return 0.0
    
    def _assign_quality_grade(self, total_score: float) -> QualityGrade:
        """ì´ì ì— ë”°ë¥¸ ë“±ê¸‰ í• ë‹¹"""
        
        if total_score >= 80:
            return QualityGrade.A_PLUS
        elif total_score >= 70:
            return QualityGrade.A
        elif total_score >= 60:
            return QualityGrade.B_PLUS
        elif total_score >= 50:
            return QualityGrade.B
        elif total_score >= 30:
            return QualityGrade.C
        else:
            return QualityGrade.D
    
    def calculate_quality_score(self, metrics: Dict[str, float]) -> float:
        """ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ë¡œë¶€í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        
        return sum(metrics.values())
    
    def evaluate_paper_set(self, papers: List[PaperInfo]) -> Dict[str, any]:
        """ë…¼ë¬¸ ì„¸íŠ¸ ì „ì²´ í‰ê°€"""
        
        if not papers:
            return {
                "average_score": 0,
                "average_grade": "N/A",
                "paper_count": 0,
                "quality_distribution": {}
            }
        
        # ê° ë…¼ë¬¸ í‰ê°€
        evaluations = [self.evaluate_paper_metrics(paper) for paper in papers]
        
        # í‰ê·  ì ìˆ˜
        average_score = sum(e.total_score for e in evaluations) / len(evaluations)
        
        # ë“±ê¸‰ ë¶„í¬
        grade_distribution = {}
        for eval in evaluations:
            grade = eval.quality_grade.value  # Enum value ì‚¬ìš©
            grade_distribution[grade] = grade_distribution.get(grade, 0) + 1
        
        # ëŒ€í‘œ ë“±ê¸‰ (í‰ê·  ì ìˆ˜ ê¸°ë°˜)
        average_grade = self._assign_quality_grade(average_score)
        
        return {
            "average_score": round(average_score, 1),
            "average_grade": average_grade,
            "paper_count": len(papers),
            "quality_distribution": grade_distribution,
            "evaluations": evaluations
        }
    
    def generate_quality_report(self, paper: PaperInfo, metrics: QualityMetrics) -> str:
        """ë…¼ë¬¸ í’ˆì§ˆ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        report = f"""
ğŸ“Š ë…¼ë¬¸ í’ˆì§ˆ í‰ê°€ ë¦¬í¬íŠ¸
{'='*50}

ğŸ“„ ë…¼ë¬¸ ì •ë³´:
- ì œëª©: {paper.title}
- ì €ì: {paper.authors}
- ì €ë„: {paper.journal} (IF: {paper.impact_factor})
- ë°œí–‰ì—°ë„: {paper.year}
- ì¸ìš©íšŸìˆ˜: {paper.citations}
- ë…¼ë¬¸ìœ í˜•: {paper.paper_type}

ğŸ“ˆ í’ˆì§ˆ í‰ê°€ ê²°ê³¼:
- ë…¼ë¬¸ ìœ í˜• ì ìˆ˜: {metrics.paper_type_score}/35ì 
- Impact Factor ì ìˆ˜: {metrics.impact_factor_score:.1f}/30ì 
- ì¸ìš© ìˆ˜ ì ìˆ˜: {metrics.citation_score:.1f}/20ì 
- ìµœì‹ ì„± ì ìˆ˜: {metrics.recency_score}/15ì 

âœ… ì´ì : {metrics.total_score:.1f}/100ì 
â­ ë“±ê¸‰: {metrics.quality_grade}

ğŸ’¡ í‰ê°€ í•´ì„:
"""
        
        # ë“±ê¸‰ë³„ í•´ì„ ì¶”ê°€
        if metrics.total_score >= 80:
            report += "- ìµœê³  ìˆ˜ì¤€ì˜ ê·¼ê±°ë¥¼ ì œê³µí•˜ëŠ” ìš°ìˆ˜í•œ ë…¼ë¬¸ì…ë‹ˆë‹¤.\n"
            report += "- ë†’ì€ ì‹ ë¢°ë„ë¡œ ì½˜í…ì¸  ìƒì„±ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif metrics.total_score >= 70:
            report += "- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ìš°ìˆ˜í•œ ê·¼ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n"
            report += "- ì•ˆì‹¬í•˜ê³  ì½˜í…ì¸  ìƒì„±ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        elif metrics.total_score >= 60:
            report += "- ì–‘í˜¸í•œ ìˆ˜ì¤€ì˜ ê·¼ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n"
            report += "- ì¶”ê°€ ë…¼ë¬¸ê³¼ í•¨ê»˜ í™œìš©í•˜ë©´ ë”ìš± ì¢‹ìŠµë‹ˆë‹¤."
        elif metrics.total_score >= 50:
            report += "- ê¸°ë³¸ì ì¸ ê·¼ê±°ëŠ” ì œê³µí•˜ë‚˜ ë³´ì™„ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
            report += "- ë‹¤ë¥¸ ê³ í’ˆì§ˆ ë…¼ë¬¸ê³¼ í•¨ê»˜ í™œìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        else:
            report += "- ê·¼ê±° ìˆ˜ì¤€ì´ ë‚®ì•„ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
            report += "- ë°˜ë“œì‹œ ë‹¤ë¥¸ ë…¼ë¬¸ë“¤ê³¼ êµì°¨ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        return report