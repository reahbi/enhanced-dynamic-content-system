#!/usr/bin/env python3
"""
Final System Verification - ìµœì¢… ì‹œìŠ¤í…œ ê²€ì¦
Enhanced Dynamic System v6.1ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ì¢…í•© ê²€ì¦
"""

import os
import json
import time
from typing import Dict, List
from datetime import datetime
from google import genai
from google.genai import types
from dotenv import load_dotenv

load_dotenv()

class SystemPerformanceAnalyzer:
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    def __init__(self):
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found")
        
        self.client = genai.Client(api_key=api_key)
        
    def analyze_system_performance(self) -> Dict:
        """ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¢…í•© ë¶„ì„"""
        
        print("ğŸ” Enhanced Dynamic System v6.1 ìµœì¢… ê²€ì¦ ì‹œì‘")
        print("=" * 60)
        
        performance_data = {
            "timestamp": datetime.now().isoformat(),
            "api_connectivity": self._test_api_connectivity(),
            "content_generation_speed": self._test_generation_speed(),
            "quality_consistency": self._test_quality_consistency(),
            "memory_efficiency": self._test_memory_usage(),
            "error_handling": self._test_error_handling(),
            "overall_score": 0.0
        }
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        scores = [
            performance_data["api_connectivity"]["score"],
            performance_data["content_generation_speed"]["score"],
            performance_data["quality_consistency"]["score"],
            performance_data["memory_efficiency"]["score"],
            performance_data["error_handling"]["score"]
        ]
        performance_data["overall_score"] = sum(scores) / len(scores)
        
        return performance_data
    
    def _test_api_connectivity(self) -> Dict:
        """API ì—°ê²°ì„± í…ŒìŠ¤íŠ¸"""
        print("\n1ï¸âƒ£ API ì—°ê²°ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        start_time = time.time()
        try:
            response = self.client.models.generate_content(
                model='gemini-2.0-flash-exp',
                contents="Hello, test message",
                config=types.GenerateContentConfig(max_output_tokens=50)
            )
            
            response_time = time.time() - start_time
            
            if response.text:
                score = 100.0 if response_time < 3.0 else max(70.0, 100 - (response_time * 10))
                result = {
                    "status": "SUCCESS",
                    "response_time": response_time,
                    "score": score,
                    "message": f"API ì—°ê²° ì„±ê³µ ({response_time:.2f}ì´ˆ)"
                }
            else:
                result = {
                    "status": "FAILED",
                    "response_time": response_time,
                    "score": 0.0,
                    "message": "ì‘ë‹µ ì—†ìŒ"
                }
                
        except Exception as e:
            result = {
                "status": "ERROR",
                "response_time": time.time() - start_time,
                "score": 0.0,
                "message": f"API ì˜¤ë¥˜: {str(e)}"
            }
        
        print(f"   âœ… {result['message']}")
        return result
    
    def _test_generation_speed(self) -> Dict:
        """ì½˜í…ì¸  ìƒì„± ì†ë„ í…ŒìŠ¤íŠ¸"""
        print("\n2ï¸âƒ£ ì½˜í…ì¸  ìƒì„± ì†ë„ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_prompt = "ìš´ë™ê³¼ ê±´ê°•ì— ê´€í•œ ê°„ë‹¨í•œ íŒ 3ê°œë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”."
        
        times = []
        for i in range(3):
            start_time = time.time()
            try:
                response = self.client.models.generate_content(
                    model='gemini-2.0-flash-exp',
                    contents=test_prompt,
                    config=types.GenerateContentConfig(max_output_tokens=500)
                )
                generation_time = time.time() - start_time
                times.append(generation_time)
                print(f"   í…ŒìŠ¤íŠ¸ {i+1}: {generation_time:.2f}ì´ˆ")
            except Exception as e:
                print(f"   í…ŒìŠ¤íŠ¸ {i+1} ì‹¤íŒ¨: {e}")
                times.append(10.0)  # í˜ë„í‹° ì‹œê°„
        
        avg_time = sum(times) / len(times)
        
        # ì ìˆ˜ ê³„ì‚°: 5ì´ˆ ì´í•˜ = 100ì , 10ì´ˆ ì´ìƒ = 50ì 
        if avg_time <= 5.0:
            score = 100.0
        elif avg_time <= 10.0:
            score = 100 - (avg_time - 5) * 10
        else:
            score = 50.0
        
        result = {
            "average_time": avg_time,
            "individual_times": times,
            "score": score,
            "evaluation": "ìš°ìˆ˜" if avg_time <= 5.0 else "ì–‘í˜¸" if avg_time <= 10.0 else "ê°œì„  í•„ìš”"
        }
        
        print(f"   âœ… í‰ê·  ìƒì„± ì‹œê°„: {avg_time:.2f}ì´ˆ ({result['evaluation']})")
        return result
    
    def _test_quality_consistency(self) -> Dict:
        """í’ˆì§ˆ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸"""
        print("\n3ï¸âƒ£ í’ˆì§ˆ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_prompts = [
            "ë‹¨ë°±ì§ˆ ì„­ì·¨ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "HIIT ìš´ë™ì˜ ì¥ë‹¨ì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.",
            "ê±´ê°•í•œ ìˆ˜ë©´ ìŠµê´€ì— ëŒ€í•œ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”."
        ]
        
        quality_scores = []
        
        for i, prompt in enumerate(test_prompts):
            try:
                response = self.client.models.generate_content(
                    model='gemini-2.0-flash-exp',
                    contents=prompt,
                    config=types.GenerateContentConfig(max_output_tokens=800)
                )
                
                # ê°„ë‹¨í•œ í’ˆì§ˆ í‰ê°€ (ê¸¸ì´, êµ¬ì¡° ë“±)
                content = response.text
                
                quality_score = 0
                # ê¸¸ì´ í‰ê°€ (ì ì ˆí•œ ê¸¸ì´ì¸ì§€)
                if 200 <= len(content) <= 1000:
                    quality_score += 30
                elif 100 <= len(content) < 200 or 1000 < len(content) <= 1500:
                    quality_score += 20
                else:
                    quality_score += 10
                
                # êµ¬ì¡° í‰ê°€ (ë¬¸ë‹¨ êµ¬ë¶„ ë“±)
                if '\n' in content:
                    quality_score += 20
                
                # ë‚´ìš© í¬í•¨ ì—¬ë¶€
                if any(keyword in content.lower() for keyword in ['ìš´ë™', 'ê±´ê°•', 'íš¨ê³¼', 'ë°©ë²•']):
                    quality_score += 30
                
                # ì „ë¬¸ì„± (êµ¬ì²´ì  ì •ë³´ í¬í•¨)
                if any(keyword in content for keyword in ['ì—°êµ¬', 'ë…¼ë¬¸', 'ì „ë¬¸ê°€', 'ì˜ë£Œ']):
                    quality_score += 20
                
                quality_scores.append(quality_score)
                print(f"   í…ŒìŠ¤íŠ¸ {i+1}: {quality_score}ì ")
                
            except Exception as e:
                print(f"   í…ŒìŠ¤íŠ¸ {i+1} ì‹¤íŒ¨: {e}")
                quality_scores.append(0)
        
        avg_quality = sum(quality_scores) / len(quality_scores)
        consistency = 100 - (max(quality_scores) - min(quality_scores))  # í¸ì°¨ê°€ ì ì„ìˆ˜ë¡ ì¼ê´€ì„± ë†’ìŒ
        
        result = {
            "average_quality": avg_quality,
            "individual_scores": quality_scores,
            "consistency": consistency,
            "score": (avg_quality + consistency) / 2,
            "evaluation": "ìš°ìˆ˜" if avg_quality >= 80 else "ì–‘í˜¸" if avg_quality >= 60 else "ê°œì„  í•„ìš”"
        }
        
        print(f"   âœ… í‰ê·  í’ˆì§ˆ: {avg_quality:.1f}ì , ì¼ê´€ì„±: {consistency:.1f}ì  ({result['evaluation']})")
        return result
    
    def _test_memory_usage(self) -> Dict:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸"""
        print("\n4ï¸âƒ£ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        try:
            import psutil
            process = psutil.Process()
            
            # í…ŒìŠ¤íŠ¸ ì „ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            memory_before = process.memory_info().rss / 1024 / 1024  # MB
            
            # ì—¬ëŸ¬ ìš”ì²­ì„ ì—°ì†ìœ¼ë¡œ ì‹¤í–‰
            for i in range(5):
                response = self.client.models.generate_content(
                    model='gemini-2.0-flash-exp',
                    contents=f"í…ŒìŠ¤íŠ¸ ìš”ì²­ {i+1}: ê°„ë‹¨í•œ ìš´ë™ íŒì„ ì•Œë ¤ì£¼ì„¸ìš”.",
                    config=types.GenerateContentConfig(max_output_tokens=300)
                )
            
            # í…ŒìŠ¤íŠ¸ í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            memory_after = process.memory_info().rss / 1024 / 1024  # MB
            memory_diff = memory_after - memory_before
            
            # ì ìˆ˜ ê³„ì‚°: ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ì´ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            if memory_diff <= 10:
                score = 100.0
            elif memory_diff <= 50:
                score = 90.0
            elif memory_diff <= 100:
                score = 70.0
            else:
                score = 50.0
            
            result = {
                "memory_before": memory_before,
                "memory_after": memory_after,
                "memory_increase": memory_diff,
                "score": score,
                "evaluation": "íš¨ìœ¨ì " if memory_diff <= 50 else "ë³´í†µ" if memory_diff <= 100 else "ê°œì„  í•„ìš”"
            }
            
        except ImportError:
            # psutilì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì ìˆ˜
            result = {
                "memory_before": "N/A",
                "memory_after": "N/A", 
                "memory_increase": "N/A",
                "score": 80.0,
                "evaluation": "ì¸¡ì • ë¶ˆê°€ (psutil í•„ìš”)"
            }
        except Exception as e:
            result = {
                "memory_before": "ERROR",
                "memory_after": "ERROR",
                "memory_increase": "ERROR",
                "score": 60.0,
                "evaluation": f"ì˜¤ë¥˜: {str(e)}"
            }
        
        print(f"   âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {result['evaluation']}")
        return result
    
    def _test_error_handling(self) -> Dict:
        """ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        print("\n5ï¸âƒ£ ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        error_tests = [
            {
                "name": "ë¹ˆ í”„ë¡¬í”„íŠ¸",
                "prompt": "",
                "expected": "ERROR_HANDLED"
            },
            {
                "name": "ë§¤ìš° ê¸´ í”„ë¡¬í”„íŠ¸",
                "prompt": "test " * 10000,
                "expected": "ERROR_HANDLED"
            },
            {
                "name": "íŠ¹ìˆ˜ ë¬¸ì",
                "prompt": "!@#$%^&*(){}[]<>?/|\\`~",
                "expected": "SUCCESS_OR_HANDLED"
            }
        ]
        
        passed_tests = 0
        
        for test in error_tests:
            try:
                response = self.client.models.generate_content(
                    model='gemini-2.0-flash-exp',
                    contents=test["prompt"],
                    config=types.GenerateContentConfig(max_output_tokens=100)
                )
                
                if test["expected"] == "SUCCESS_OR_HANDLED":
                    passed_tests += 1
                    print(f"   âœ… {test['name']}: ì²˜ë¦¬ë¨")
                else:
                    print(f"   âš ï¸ {test['name']}: ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²°ê³¼")
                    
            except Exception as e:
                if test["expected"] == "ERROR_HANDLED":
                    passed_tests += 1
                    print(f"   âœ… {test['name']}: ì˜¤ë¥˜ ì ì ˆíˆ ì²˜ë¦¬ë¨")
                else:
                    print(f"   âŒ {test['name']}: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ - {str(e)[:50]}")
        
        score = (passed_tests / len(error_tests)) * 100
        
        result = {
            "total_tests": len(error_tests),
            "passed_tests": passed_tests,
            "score": score,
            "evaluation": "ìš°ìˆ˜" if score >= 80 else "ì–‘í˜¸" if score >= 60 else "ê°œì„  í•„ìš”"
        }
        
        print(f"   âœ… ì˜¤ë¥˜ ì²˜ë¦¬: {passed_tests}/{len(error_tests)} í…ŒìŠ¤íŠ¸ í†µê³¼ ({result['evaluation']})")
        return result

def main():
    """ë©”ì¸ ê²€ì¦ í•¨ìˆ˜"""
    try:
        analyzer = SystemPerformanceAnalyzer()
        performance_data = analyzer.analyze_system_performance()
        
        print("\n" + "=" * 60)
        print("ğŸ¯ ìµœì¢… ê²€ì¦ ê²°ê³¼")
        print("=" * 60)
        
        print(f"ğŸ“Š ì¢…í•© ì ìˆ˜: {performance_data['overall_score']:.1f}ì ")
        
        if performance_data['overall_score'] >= 90:
            grade = "A+ (ìµœìƒê¸‰)"
        elif performance_data['overall_score'] >= 80:
            grade = "A (ìš°ìˆ˜)"
        elif performance_data['overall_score'] >= 70:
            grade = "B (ì–‘í˜¸)"
        elif performance_data['overall_score'] >= 60:
            grade = "C (ë³´í†µ)"
        else:
            grade = "D (ê°œì„  í•„ìš”)"
        
        print(f"ğŸ† ì‹œìŠ¤í…œ ë“±ê¸‰: {grade}")
        
        print("\nğŸ“ˆ ì„¸ë¶€ í•­ëª©ë³„ ì ìˆ˜:")
        print(f"   â€¢ API ì—°ê²°ì„±: {performance_data['api_connectivity']['score']:.1f}ì ")
        print(f"   â€¢ ìƒì„± ì†ë„: {performance_data['content_generation_speed']['score']:.1f}ì ")
        print(f"   â€¢ í’ˆì§ˆ ì¼ê´€ì„±: {performance_data['quality_consistency']['score']:.1f}ì ")
        print(f"   â€¢ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {performance_data['memory_efficiency']['score']:.1f}ì ")
        print(f"   â€¢ ì˜¤ë¥˜ ì²˜ë¦¬: {performance_data['error_handling']['score']:.1f}ì ")
        
        # ê²°ê³¼ ì €ì¥
        os.makedirs("test_results", exist_ok=True)
        with open("test_results/final_verification.json", 'w', encoding='utf-8') as f:
            json.dump(performance_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ test_results/final_verification.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print("\nâœ… Enhanced Dynamic System v6.1 ê²€ì¦ ì™„ë£Œ!")
        
        if performance_data['overall_score'] >= 80:
            print("ğŸš€ ì‹œìŠ¤í…œì´ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print("ğŸ”§ ì¼ë¶€ ì˜ì—­ì—ì„œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()